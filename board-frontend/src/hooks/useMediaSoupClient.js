import { useEffect, useRef, useState } from 'react';
import io from 'socket.io-client';
import * as mediasoupClient from 'mediasoup-client';

const SERVER_URL = 'http://localhost:4000';

export default function useMediasoupClient(userId, nickname) {
  const socketRef = useRef(null);
  const deviceRef = useRef(null);
  const sendTransportRef = useRef(null);
  const recvTransportRef = useRef(null);
  const producerRef = useRef(null);
  const currentChannelIdRef = useRef(null);
  const participantsRef = useRef(new Map());
  const consumerRefs = useRef([]);
  const streamRef = useRef(null);
  const micIntervalRef = useRef(null);
  const [micVolume, setMicVolume] = useState(0); // 0~100
const [speakingUserIds, setSpeakingUserIds] = useState(new Set());
  const [voiceParticipantsMap, setVoiceParticipantsMap] = useState(new Map());

  const iceServers = [
    {
      urls: "turn:127.0.0.1:3478",
      username: "testuser",
      credential: "testpass"
    }
  ];

  useEffect(() => {
    socketRef.current = io(SERVER_URL);

    socketRef.current.on('connect', async () => {
      console.log('âœ… Connected to mediasoup server');
      console.log("ğŸ“Œ register emit:", { userId, nickname });
      socketRef.current.emit('register', { userId, nickname });

      const device = new mediasoupClient.Device();
      deviceRef.current = device;

      socketRef.current.emit('getRtpCapabilities', async (rtpCapabilities) => {
        console.log("ğŸ¯ RTP Capabilities:", rtpCapabilities.codecs.map(c => c.mimeType));
        await device.load({ routerRtpCapabilities: rtpCapabilities });
        console.log('ğŸ“¡ Device loaded');
      });
    });

    socketRef.current.on('voiceParticipantsUpdate', ({ channelId, participants }) => {
      participantsRef.current.set(channelId, participants);
      setVoiceParticipantsMap(new Map(participantsRef.current));
    });

    socketRef.current.on('newProducer', async ({ producerId }) => {
      console.log("ğŸ§ New producer detected:", producerId);
      await consumeSpecificAudio(producerId);
    });

    return () => socketRef.current.disconnect();
  }, [userId,nickname]);

  const joinVoiceChannel = async (newChannelId) => {
    if (currentChannelIdRef.current && currentChannelIdRef.current !== newChannelId) {
      // ê¸°ì¡´ ì±„ë„ ë¨¼ì € ì •ë¦¬
      await leaveVoiceChannel();
    }
  
    // ìƒˆ ì±„ë„ ì¡°ì¸
    socketRef.current.emit('joinVoiceChannel', { channelId: newChannelId });
    currentChannelIdRef.current = newChannelId;
  };

  const leaveVoiceChannel = async () => {
    if (currentChannelIdRef.current) {
      socketRef.current.emit('leaveVoiceChannel', { channelId: currentChannelIdRef.current });
      currentChannelIdRef.current = null;
    }
  
    try {
      producerRef.current?.close();
    } catch (e) {}
    producerRef.current = null;
  
    try {
      sendTransportRef.current?.close();
    } catch (e) {}
    sendTransportRef.current = null;
  
    try {
      recvTransportRef.current?.close();
    } catch (e) {}
    recvTransportRef.current = null;
  
    consumerRefs.current.forEach((c) => {
      try { c.close(); } catch {}
    });
    consumerRefs.current = [];
  
    // âœ… ë§ˆì´í¬ ì¸¡ì • íƒ€ì´ë¨¸ ì •ë¦¬
    if (micIntervalRef.current) {
      clearInterval(micIntervalRef.current);
      micIntervalRef.current = null;
    }
  
    // stream ì •ë¦¬
    if (streamRef.current) {
      streamRef.current.getTracks().forEach((t) => t.stop());
      streamRef.current = null;
    }
  
    // DOM ì˜¤ë””ì˜¤ ì œê±°
    document.querySelectorAll("audio").forEach((audio) => {
      if (audio.srcObject instanceof MediaStream) {
        audio.pause();
        audio.srcObject = null;
        audio.remove();
      }
    });
  };

  const createSendTransport = async () => {
    return new Promise((resolve, reject) => {
      socketRef.current.emit('createWebRtcTransport', { direction: 'send' }, async (params) => {
        console.log("ğŸšš createWebRtcTransport ì‘ë‹µ:", params);
        if (params.error) {
          console.error("âŒ Transport ìƒì„± ì‹¤íŒ¨:", params.error);
          return reject(params.error);
        }
      
        try {
          const transport = deviceRef.current.createSendTransport({
            ...params,
            iceServers: []
          });
          sendTransportRef.current = transport;

          transport.on('connect', ({ dtlsParameters }, callback, errback) => {
            socketRef.current.emit('connectTransport', { transportId: transport.id, dtlsParameters }, (res) => {
              res.error ? errback(res.error) : callback();
            });
          });

          transport.on('produce', ({ kind, rtpParameters }, callback, errback) => {
            socketRef.current.emit('produce', { transportId: transport.id, kind, rtpParameters }, ({ id, error }) => {
              error ? errback(error) : callback({ id });
            });
          });

          resolve();
        } catch (err) {
          reject(err);
        }
      });
    });
  };

  const sendAudio = async () => {
    console.log("ğŸ¤ sendAudio called");
  
    if (!sendTransportRef.current) {
      console.warn("âŒ sendTransportRef is null");
      return;
    }
  
    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: false,
          noiseSuppression: false,
          autoGainControl: false
        }
      });
      const track = stream.getAudioTracks()[0];
      console.log("ğŸ™ï¸ Got local audio track:", track.label);
  
      console.log("âœ… sendTransport connectionState:", sendTransportRef.current.connectionState);
      console.log("âœ… track.readyState:", track.readyState);
      console.log("âœ… track.enabled:", track.enabled);
      console.log("âœ… track.muted:", track.muted);
  
      sendTransportRef.current.on('connectionstatechange', (state) => {
        console.log(`ğŸš© sendTransport connectionstatechange: ${state}`);
      });
  
      const audioContext = new (window.AudioContext || window.webkitAudioContext)();
      const source = audioContext.createMediaStreamSource(stream);
      const analyser = audioContext.createAnalyser();
      source.connect(analyser);
      const dataArray = new Uint8Array(analyser.frequencyBinCount);
  
      // ì´ì „ íƒ€ì´ë¨¸ ì œê±°
      if (micIntervalRef.current) {
        clearInterval(micIntervalRef.current);
        micIntervalRef.current = null;
      }
  
      // ë§ˆì´í¬ ë ˆë²¨ ì¸¡ì • ì‹œì‘
      micIntervalRef.current = setInterval(() => {
        analyser.getByteFrequencyData(dataArray);
        const avg = dataArray.reduce((a, b) => a + b, 0) / dataArray.length;
        setMicVolume(avg); //  ë‚´ ë§ˆì´í¬ ê²Œì´ì§€ìš©

        const threshold = 10; // ë§í•˜ëŠ” ê¸°ì¤€ ë³¼ë¥¨
      
        // ìì‹ ì„ ë§í•˜ëŠ” ì‚¬ëŒìœ¼ë¡œ ë“±ë¡
        if (avg > threshold) {
          setSpeakingUserIds((prev) => {
            const newSet = new Set(prev);
            newSet.add(userId);
            return newSet;
          });
        } else {
          setSpeakingUserIds((prev) => {
            if (prev.has(userId)) {
              const newSet = new Set(prev);
              newSet.delete(userId);
              return newSet;
            }
            return prev;
          });
        }
        console.log("ğŸ™ï¸ Mic volume level:", Math.round(avg));
      }, 500);
  
      await sendTransportRef.current.produce({ track });
      console.log("ğŸ“¤ Audio produced successfully");
    } catch (err) {
      console.error("âŒ sendAudio error:", err);
    }
  };

  const createRecvTransport = async () => {
    return new Promise((resolve, reject) => {
      socketRef.current.emit('createWebRtcTransport', { direction: 'recv' }, async (params) => {
        try {
          const transport = deviceRef.current.createRecvTransport({
            ...params,
            iceServers: []
          });
          recvTransportRef.current = transport;

          transport.on('connect', ({ dtlsParameters }, callback, errback) => {
            socketRef.current.emit('connectTransport', { transportId: transport.id, dtlsParameters }, (res) => {
              res.error ? errback(res.error) : callback();
            });
          });

          socketRef.current.emit('getProducers', async (producerIds) => {
            for (const producerId of producerIds) {
              console.log("ğŸ“¦ Found existing producer:", producerId);
              await consumeSpecificAudio(producerId);
            }
          });

          resolve();
        } catch (err) {
          reject(err);
        }
      });
    });
  };

  const consumeSpecificAudio = async (producerId) => {
    if (!recvTransportRef.current) {
      console.warn("âš ï¸ recvTransportRef is null at consume time");
      return;
    }

    console.log("ğŸ”„ Attempting to consume audio from producer:", producerId);

    socketRef.current.emit(
      'consume',
      {
        transportId: recvTransportRef.current.id,
        producerId,
        rtpCapabilities: deviceRef.current.rtpCapabilities,
      },
      async (params) => {
        if (params.error) return console.error("âŒ consume error:", params.error);

        const { id, kind, rtpParameters } = params;

        console.log("ğŸ§ Consumed codec mimeType:", rtpParameters.codecs?.[0]?.mimeType);

        try {
          const consumer = await recvTransportRef.current.consume({
            id,
            producerId,
            kind,
            rtpParameters,
          });
          consumerRefs.current.push(consumer);

          console.log("ğŸ‘‚ consumer.track:", consumer.track);
          console.log('ğŸ‘‚ consumer.track.muted:', consumer.track.muted);
          console.log("ğŸ¤ consumer.track.enabled:", consumer.track.enabled);
          console.log('ğŸ‘‚ consumer.rtpParameters:', consumer.rtpParameters);

          const stream = new MediaStream();
          stream.addTrack(consumer.track);

          console.log("ğŸ“¦ MediaStream tracks:", stream.getTracks());
          console.log("ğŸ§ stream.getAudioTracks():", stream.getAudioTracks());

          const audioContext = new (window.AudioContext || window.webkitAudioContext)();
          console.log("ğŸ”Š AudioContext state (ì´ˆê¸°):", audioContext.state);

          audioContext.onstatechange = () => {
            console.log("ğŸ“» AudioContext state changed to:", audioContext.state);
          };

          try {
            await audioContext.resume();
            console.log("ğŸŸ¢ AudioContext resumed");
          } catch (e) {
            console.warn("âŒ AudioContext resume ì‹¤íŒ¨:", e);
          }

          const audio = new Audio();
          audio.srcObject = stream;
          audio.autoplay = true;
          audio.muted = false;
          audio.volume = 0.8;

          document.body.appendChild(audio);

          audio.addEventListener("playing", () => {
            console.log("ğŸ”ˆ actually playing");
          });

          audio.addEventListener("error", (e) => {
            console.warn("â— Audio playback error:", e);
          });

          console.log("ğŸ”Š Audio element ìƒíƒœ:", audio);

          try {
            await audio.play();
            console.log("ğŸ”Š Audio playback started for producer:", producerId);
          } catch (err) {
            console.warn("ğŸ”‡ Audio playback failed:", err);
          }
        } catch (err) {
          console.error("âŒ Failed to create consumer or audio:", err);
        }
      }
    );
  };

  return {
    createSendTransport,
    sendAudio,
    createRecvTransport,
    consumeSpecificAudio,
    joinVoiceChannel,
    leaveVoiceChannel,
    voiceParticipantsMap,
    speakingUserIds,
    micVolume,
  };
}
